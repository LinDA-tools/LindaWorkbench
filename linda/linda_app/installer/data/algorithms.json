[
{
  "fields": {
    "category": 1,
    "name": "J48",
    "description": "<input style=\"float:right;margin: 10px;\" src=\"/static/images/Decision_tree_model.png\"  type=\"image\">\r\n\r\n<div style=\"text-align: justify;\">\r\nJ48  implements Quinlans C4.5 algorithm  for generating a pruned or unpruned C4.5 decision tree. The decision trees generated by J48 can be used for classification. J48 builds decision trees from a set of labeled training data using the concept of information entropy. It uses the fact that each attribute of the data can be used to make a decision by splitting the data into smaller subsets. \r\n<br/><br/>\r\nJ48 examines the normalized information gain (difference in entropy) that results from choosing an attribute for splitting the data. To make the decision, the attribute with the highest normalized information gain is used. Then the algorithm recurs on the smaller subsets. The splitting procedure stops if all instances in a subset belong to the same class. Then a leaf node is created in the decision tree telling to choose that class.\r\n<br/><br/>\r\n\r\nFor More Info: <a href=\"http://www.academia.edu/4375403/Decision_Tree_Analysis_on_J48_Algorithm_for_Data_Mining\">http://www.academia.edu/4375403/Decision_Tree_Analysis_on_J48_Algorithm_for_Data_Mining</a>\r\n</div>"
  },
  "model": "analytics.algorithm",
  "pk": 1
},
{
  "fields": {
    "category": 1,
    "name": "M5P",
    "description": "<input style=\"float:right;margin: 10px;\" src=\"/static/images/Decision_tree_model.png\"  type=\"image\">\r\n\r\n<div style=\"text-align: justify;\">\r\nImplements base routines for generating M5 Model trees and rules.\r\nThe original algorithm M5 was invented by R. Quinlan and Yong Wang made improvements.\r\nThe M5 algorithm builds trees whose leaves are associated to multivariate linear\r\nmodels and the nodes of the tree are chosen over the attribute that maximizes\r\nthe expected error reduction as a function of the standard deviation of output\r\nparameter.\r\n<br/><br/>\r\n\r\nFor More Info: <a href=\"http://rapid-i.com/wiki/index.php?title=Weka:W-M5P\">http://rapid-i.com/wiki/index.php?title=Weka:W-M5P</a>\r\n</div>\r\n\r\n"
  },
  "model": "analytics.algorithm",
  "pk": 2
},
{
  "fields": {
    "category": 2,
    "name": "Apriori",
    "description": "<input style=\"float:right;margin: 10px;\" src=\"/static/images/association_category.png\"  type=\"image\">\r\n\r\n<div style=\"text-align: justify;\">\r\nApriori  is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.\r\n\r\n</br><br/>\r\nFor More Info: <a href=\"http://en.wikipedia.org/wiki/Apriori_algorithm\">http://en.wikipedia.org/wiki/Apriori_algorithm</a>\r\n</div>"
  },
  "model": "analytics.algorithm",
  "pk": 3
},
{
  "fields": {
    "category": 3,
    "name": "LinearRegression",
    "description": "\r\n<input style=\"float:right;margin: 10px;\" src=\"/static/images/regression_category.png\"  type=\"image\">\r\n\r\n<div style=\"text-align: justify;\">\r\nIn statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. (This term should be distinguished from multivariate linear regression, where multiple correlated dependent variables are predicted,[citation needed] rather than a single scalar variable.)\r\n<br/><br/>\r\nIn linear regression, data are modeled using linear predictor functions, and unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, linear regression refers to a model in which the conditional mean of y given the value of X is an affine function of X. Less commonly, linear regression could refer to a model in which the median, or some other quantile of the conditional distribution of y given X is expressed as a linear function of X. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of y given X, rather than on the joint probability distribution of y and X, which is the domain of multivariate analysis.\r\n<br/><br/>\r\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications.[citation needed] This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\r\n<br/><br/>\r\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\r\n\r\nIf the goal is prediction, or forecasting, or reduction, linear regression can be used to fit a predictive model to an observed data set of y and X values. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a prediction of the value of y.\r\nGiven a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj may have no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\r\n</br><br/>\r\nFor More Info: <a href=\"http://en.wikipedia.org/wiki/Regression_analysis\">http://en.wikipedia.org/wiki/Regression_analysis</a>\r\n</div>\r\n"
  },
  "model": "analytics.algorithm",
  "pk": 4
},
{
  "fields": {
    "category": 4,
    "name": "Arima",
    "description": " ARIMA stands for Autoregressive Integrated Moving Average models. Univariate (single vector) ARIMA is a forecasting technique that projects the future values of a series based entirely on its own inertia. Its main application is in the area of short term forecasting requiring at least 40 historical data points. It works best when your data exhibits a stable or consistent pattern over time with a minimum amount of outliers.ARIMA is usually superior to exponential smoothing techniques when the data is reasonably long and the correlation between past observations is stable. \r\n</br>\r\nFor More Info: <a href=\"http://www.forecastingsolutions.com/arima.html\">http://www.forecastingsolutions.com/arima.html</a>\r\n\r\n<h2>Tips:</h2>\r\n1. If you do not have at least 38 data points, you should consider some other method than ARIMA.</br>\r\n2. Your data should contain a \"date\" column. if not, please don't forget to give the startDate & endDate parameters.</br>\r\n3. The value to predict should be always placed as the last column of your query or tabular data file.</br>\r\n"
  },
  "model": "analytics.algorithm",
  "pk": 5
},
{
  "fields": {
    "category": 5,
    "name": "Morans I",
    "description": "In statistics, Morans I is a measure of spatial autocorrelation developed by Patrick Alfred Pierce Moran. Spatial autocorrelation is characterized by a correlation in a signal among nearby locations in space. Spatial autocorrelation is more complex than one-dimensional autocorrelation because spatial correlation is multi-dimensional \r\n(i.e. 2 or 3 dimensions of space) and multi-directional. Negative (positive) values indicate negative (positive) spatial autocorrelation. Values range from -1 (indicating perfect dispersion) to +1 (perfect correlation).  A zero value indicates a random spatial pattern. Morans I is a measure of global spatial autocorrelation.\r\n\r\nMoran's I is a measure of spatial autocorrelation--how related the values of a variable are based on the locations where they were measured.  To calculate Moran's I in R are used functions of the ape library.\r\n\r\n\r\n<h2>Tips:</h2>\r\n1. The value to calculate moran's I metrics should be always placed as the last column of your query or tabular data file.</br>\r\n2. Your data should contain a \"x\" and \"y\" column. \"x\" refers to longtitude and \"y\" to latitude. if not, please don't forget to give the x & y parameters.\r\n"
  },
  "model": "analytics.algorithm",
  "pk": 15
},
{
  "fields": {
    "category": 5,
    "name": "Kriging",
    "description": "Spatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran's I or Geary's c) against distance. Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\r\n\r\n<h2>Tips:</h2>\r\n1. The value to visualize in the correlogram should be always placed as the last column of your query or tabular data file.</br>\r\n2. Your data should contain a \"x\" and \"y\" column. \"x\" refers to longtitude and \"y\" to latitude. if not, please don't forget to give the x & y parameters.\r\n\r\n"
  },
  "model": "analytics.algorithm",
  "pk": 16
},
{
  "fields": {
    "category": 5,
    "name": "NCF correlogram",
    "description": "Spatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran's I or Geary's c) against distance. Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\r\n\r\n<h2>Tips:</h2>\r\n1. The value to visualize in the correlogram should be always placed as the last column of your query or tabular data file.</br>\r\n2. Your data should contain a \"x\" and \"y\" column. \"x\" refers to longtitude and \"y\" to latitude. if not, please don't forget to give the x & y parameters.\r\n\r\n"
  },
  "model": "analytics.algorithm",
  "pk": 17
},
{
  "fields": {
    "category": 6,
    "name": "ClustersNumber",
    "description": "ClustersNumber"
  },
  "model": "analytics.algorithm",
  "pk": 18
},
{
  "fields": {
    "category": 6,
    "name": "KMeans",
    "description": "<input style=\"float:right;margin: 10px;\" src=\"/static/images/KMeans.png\"  type=\"image\">\r\n<div style=\"text-align: justify;\">\r\n<p><strong>K-means</strong> clustering is the most popular partitioning method. It requires the analyst to specify the number of clusters to extract. A plot of the within groups sum of squares by number of clusters extracted can help determine the appropriate number of clusters. The analyst looks for a bend in the plot similar to a scree test in factor analysis. See <a href=\"http://www.statmethods.net/about/books.html\">Everitt &amp; Hothorn (pg. 251)</a>. </p>\r\n</br><br/>\r\n<h2>Tips:</h2>\r\n1. All given values shoud be numeric.</br>\r\n2. Your data should contain an  \"uri\" column. if not, please don't forget to give the uri parameter.\r\n</br><br/>\r\n\r\nFor More Info: <a href=\"http://www.statmethods.net/advstats/cluster.html\">http://www.statmethods.net/advstats/cluster.html</a>\r\n</div>\r\n\r\n\r\n"
  },
  "model": "analytics.algorithm",
  "pk": 19
},
{
  "fields": {
    "category": 6,
    "name": "Ward Hierarchical Agglomerative",
    "description": "\r\n<input style=\"float:right;margin: 10px;\" src=\"/static/images/dentrogram.png\"  type=\"image\">\r\n\r\n<div style=\"text-align: justify;\">\r\nIn data mining, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:[citation needed]\r\n\r\nAgglomerative: This is a \"bottom up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\r\nDivisive: This is a \"top down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\r\nIn general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\r\n</br><br/>\r\nFor More Info: <a href=\"http://en.wikipedia.org/wiki/Hierarchical_clustering\">http://en.wikipedia.org/wiki/Hierarchical_clustering</a>\r\n</div>\r\n\r\n\r\n"
  },
  "model": "analytics.algorithm",
  "pk": 20
},
{
  "fields": {
    "category": 6,
    "name": "Model Based Clustering",
    "description": "\r\n<input style=\"float:right;margin: 10px;\" src=\"/static/images/BMClustering.png\"  type=\"image\">\r\n\r\n<div style=\"text-align: justify;\">\r\nModel based approaches assume a variety of data models and apply maximum likelihood estimation and Bayes criteria to identify the most likely model and number of clusters. Specifically, the Mclust( ) function in the mclust package selects the optimal model according to BIC for EM initialized by hierarchical clustering for parameterized Gaussian mixture models. One chooses the model and number of clusters with the largest BIC. See help(mclustModelNames) to details on the model chosen as best.\r\n</br></br>\r\nFor More Info: <a href=\"http://en.wikipedia.org/wiki/Cluster_analysis\">http://en.wikipedia.org/wiki/Cluster_analysis</a>\r\n</div>"
  },
  "model": "analytics.algorithm",
  "pk": 21
}
]
